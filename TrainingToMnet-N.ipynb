{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6+ocJOpQ0yKwiXOs73hJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nik-Kras/ToMnet-N/blob/main/TrainingToMnet-N.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Initialise classes for Networks, Loading and Processing"
      ],
      "metadata": {
        "id": "0FxY_f1h_hZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layers"
      ],
      "metadata": {
        "id": "nnanqeIq_4yi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WDSLrSJt_L9S"
      },
      "outputs": [],
      "source": [
        "### LAYERS\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "class NeuralNetLayers:\n",
        "\n",
        "The parent class for both the character net and the prediction net.\n",
        "@author: Chuang, Yun-Shiuan\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LSTM\n",
        "from keras import activations\n",
        "\n",
        "class CustomCnn(keras.layers.Layer):\n",
        "    def __init__(self, input_tensor=None, activation=\"linear\", filters=32,\n",
        "                 UseTimeWrapper=False, regularisation_value = 0.001, **kwargs):\n",
        "        super(CustomCnn, self).__init__(**kwargs)\n",
        "        self.UseTimeWrapper = UseTimeWrapper\n",
        "        self.input_tensor = input_tensor\n",
        "        self.activation = activation\n",
        "        self.filters = filters\n",
        "        self.regularisation_value = regularisation_value\n",
        "\n",
        "        if input_tensor is None:\n",
        "            self.conv = tf.keras.layers.Conv2D(filters=filters,\n",
        "                                                kernel_size=(3, 3),\n",
        "                                                strides=(1, 1),\n",
        "                                                activation=activation,\n",
        "                                                padding=\"same\",\n",
        "                                                kernel_regularizer = keras.regularizers.l2(self.regularisation_value),\n",
        "                                                bias_regularizer = keras.regularizers.l2(self.regularisation_value),\n",
        "                                                kernel_initializer = tf.keras.initializers.HeNormal())\n",
        "        else:\n",
        "            self.conv = tf.keras.layers.Conv2D(filters=filters,\n",
        "                                                kernel_size=(3, 3),\n",
        "                                                strides=(1, 1),\n",
        "                                                activation=activation,\n",
        "                                                padding=\"same\",\n",
        "                                                input_shape=input_tensor,\n",
        "                                                kernel_regularizer = keras.regularizers.l2(self.regularisation_value),\n",
        "                                                bias_regularizer = keras.regularizers.l2(self.regularisation_value),\n",
        "                                                kernel_initializer = tf.keras.initializers.HeNormal())\n",
        "        if UseTimeWrapper: self.conv_handler = tf.keras.layers.TimeDistributed(self.conv)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.UseTimeWrapper: x = self.conv_handler(inputs)\n",
        "        else: x = self.conv(inputs)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"regularisation_value\": self.regularisation_value,\n",
        "            \"UseTimeWrapper\": self.UseTimeWrapper,\n",
        "            \"input_tensor\": self.input_tensor,\n",
        "            \"activation\": self.activation,\n",
        "            \"filters\": self.filters,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "def CustomCnnCharNet(input_tensor=None, activation=\"linear\", filters=32, **kwargs):\n",
        "    return CustomCnn(input_tensor=input_tensor, activation=activation, filters=filters, UseTimeWrapper=True, **kwargs)\n",
        "\n",
        "def CustomCnnPredNet(input_tensor=None, activation=\"linear\", filters=32, **kwargs):\n",
        "    return CustomCnn(input_tensor=input_tensor, activation=activation, filters=filters, UseTimeWrapper=False, **kwargs)\n",
        "\n",
        "class ResBlock(keras.layers.Layer):\n",
        "    def __init__(self, UseTimeWrapper=False, filters=32, **kwargs):\n",
        "        super(ResBlock, self).__init__(**kwargs)\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.relu_conv = tf.keras.layers.Activation('relu')\n",
        "        self.UseTimeWrapper = UseTimeWrapper\n",
        "        if self.UseTimeWrapper:\n",
        "            self.conv1 = CustomCnnCharNet(activation=\"linear\", filters=filters)\n",
        "            self.conv2 = CustomCnnCharNet(activation=\"linear\", filters=filters)\n",
        "        else:\n",
        "            self.conv1 = CustomCnnPredNet(activation=\"linear\", filters=filters)\n",
        "            self.conv2 = CustomCnnPredNet(activation=\"linear\", filters=filters)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu_conv(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = tf.nn.relu(x + inputs)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "          \"UseTimeWrapper\": self.UseTimeWrapper\n",
        "      })\n",
        "      return config\n",
        "\n",
        "def ResBlockCharNet(filters=32):\n",
        "    return ResBlock(UseTimeWrapper=True, filters=filters)\n",
        "\n",
        "def ResBlockPredNet(filters=32):\n",
        "    return ResBlock(UseTimeWrapper=False, filters=filters)\n",
        "\n",
        "class CustomLSTM(keras.layers.Layer):\n",
        "    def __init__(self, num_hidden = 128,  **kwargs):\n",
        "        super(CustomLSTM, self).__init__(**kwargs)  # including name = name\n",
        "        self.num_hidden = num_hidden\n",
        "        self.lstm = LSTM(units=num_hidden,\n",
        "                        activation = activations.tanh,\n",
        "                        recurrent_activation = activations.sigmoid)\n",
        "        self.bn = BatchNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.lstm(inputs)\n",
        "        # x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"num_hidden\": self.num_hidden\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CharNet"
      ],
      "metadata": {
        "id": "-fVskE2o_2jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CharNet\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "class CharNet(nnl.NeuralNetLayers):\n",
        "\n",
        "For the single trajectory τi in the past episode, the\n",
        "ToMnet forms the character embedding echar,i as follows. We\n",
        " (1) pre-process the data from each time-step by spatialising the actions,\n",
        " a(obs), concatenating these with the respective states, x(obs),\n",
        " (2) passing through a 5-layer resnet, with 32 channels, ReLU nonlinearities,\n",
        " and batch-norm, followed by average pooling.\n",
        " (3) We pass the results through an LSTM with 64 channels,\n",
        " with a linear output to either a 2-dim or 8-dim echar,i (no substantial difference in results).\n",
        "@author: Chuang, Yun-Shiuan; Edwinn\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# CharNet is a layer, as it doesn't have separate and own training,\n",
        "# it is simply a part of whole network, so can be considered as a layer\n",
        "# --------------------------------------------------------------\n",
        "class CharNet(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, input_tensor, n, N_echar, filters=32):\n",
        "        super(CharNet, self).__init__()\n",
        "\n",
        "        # self.input_tensor = input_tensor\n",
        "        self.n = n\n",
        "        self.N_echar = N_echar\n",
        "\n",
        "        self.conv = CustomCnnCharNet(input_tensor=input_tensor, filters=filters)\n",
        "        self.res_blocks = [None] * n\n",
        "        for i in range(n):\n",
        "            self.res_blocks[i] = ResBlockCharNet(filters=filters)\n",
        "        # Global Pool\n",
        "        self.lstm = CustomLSTM()\n",
        "        self.e_char = Dense(N_echar)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Build the character net.\n",
        "        \"\"\"\n",
        "\n",
        "        # input_tensor = self.input_tensor\n",
        "        n = self.n\n",
        "        N_echar = self.N_echar\n",
        "\n",
        "        batch_size, trajectory_size, height, width, depth = inputs.get_shape().as_list()\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 10, 12, 12, 11) -> (16, 10, 12, 12, 32)\n",
        "        # Add initial Conv2D layer\n",
        "        # Conv2D standard: Shape = (batch_size, width, height, channels)\n",
        "        # Conv2D takes only width x height x channels (12, 12, 11)\n",
        "        # Time Distributed layer feeds a Conv2D with time-frames (10 frames)\n",
        "        # That process is happening in parallel for 16 objects in one batch\n",
        "        # --------------------------------------------------------------\n",
        "        x = self.conv(inputs)\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 10, 12, 12, 11) -> (16, 10, 12, 12, 32)\n",
        "        # Add n residual layers\n",
        "        # Conv2D takes only width x height x channels (12, 12, 11)\n",
        "        # Time Distributed layer feeds a Conv2D with time-frames (10 frames)\n",
        "        # That process is happening in parallel for 16 objects in one batch\n",
        "        # --------------------------------------------------------------\n",
        "        for i in range(n):\n",
        "            x = self.res_blocks[i](x)  ### Possible error here!!!\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 10, 12, 12, 32) ->  (16, 10, 32)\n",
        "        # Add average pooling\n",
        "        # Collapse the spatial dimensions\n",
        "        # --------------------------------------------------------------\n",
        "        x = tf.reduce_mean(input_tensor=x, axis=[2, 3])\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 10, 32) ->  (16, 64)\n",
        "        # Add LSTM\n",
        "        # Standard: Shape = (batch_size, time_step, features)\n",
        "        # for each x_i(t)(example_i's step_t): a (64, 1) = W(64, 32) * x (32, 1)\n",
        "        # --------------------------------------------------------------\n",
        "        x = self.lstm(x)\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 64) -> (16, 4)\n",
        "        # Add Fully connected layer\n",
        "        # (batch_size, features) - > (batch_size, e_char)\n",
        "        # --------------------------------------------------------------\n",
        "        x = self.e_char(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OL5whsyl_RF_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PredNet\n"
      ],
      "metadata": {
        "id": "Q9ohkA9L_0CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### PredNet\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "class PredNet(nnl.NeuralNetLayers):\n",
        "\n",
        "In this and subsequent experiments,\n",
        "we make three predictions:\n",
        "  (1)next-step action,\n",
        "  (2)which objects are consumed by the end of the episode, and\n",
        "  (3) successor representations.\n",
        "  We use a shared torso for these predictions, from which separate heads branch off.\n",
        "\n",
        "  For the prediction torso, we\n",
        "    (1) spatialise echar,i,\n",
        "    (2) and concatenate with the query state;\n",
        "    (3) this is passed into a 5-layer resnet, with 32 channels, ReLU nonlinearities, and batch-norm.\n",
        "\n",
        "  Consumption prediction head.\n",
        "  From the torso output:\n",
        "    （1) a 1-layer convnet with 32 channels and ReLUs, followed by average pooling, and\n",
        "     (2) a fully-connected layer to 4-dims,\n",
        "     (3) followed by a sigmoid. This gives the respective Bernoulli probabilities\n",
        "     that each of the four objects will be consumed by the end of the episode.\n",
        "     [Unlike the paper, I replaced this sigmoid unit by a softmax unit.]\n",
        "@author: Chuang, Yun-Shiuan\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# CharNet is a layer, as it doesn't have separate and own training,\n",
        "# it is simply a part of whole network, so can be considered as a layer\n",
        "# --------------------------------------------------------------\n",
        "class PredNet(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, n, filters=32):\n",
        "        super(PredNet, self).__init__()\n",
        "        self.n = n\n",
        "\n",
        "        self.e_char_shape = 8\n",
        "        self.current_state_shape = (12, 12, 6)\n",
        "\n",
        "        self.conv_1 = CustomCnnPredNet(input_tensor=self.current_state_shape, filters=filters)\n",
        "        self.res_blocks = [None] * n\n",
        "        for i in range(n):\n",
        "          self.res_blocks[i] = ResBlockPredNet(filters=filters)\n",
        "        self.conv_2 = CustomCnnPredNet(activation='relu', filters=filters)\n",
        "        self.fc1 = Dense(units=128, activation=activations.relu)\n",
        "        self.fc2 = Dense(units=128, activation=activations.relu)\n",
        "        self.fc3 = Dense(units=64, activation=activations.relu)\n",
        "        # drop_out_1 = Dropout(rate = 0.2) ### Could be added in the future\n",
        "        self.goal_predict = Dense(units=4, activation=activations.softmax)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Build the character net.\n",
        "        \"\"\"\n",
        "        ### Check that inputs.shape == (None, 13, 12, 8)\n",
        "\n",
        "        # Get shapes\n",
        "        # batch_size, height, width, depth = inputs.get_shape().as_list()\n",
        "        # _, embedding_length = e_char.get_shape().as_list()\n",
        "        n = self.n\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 13, 12, 8) -> (16, 12, 12, 6) + (16, 8)  # OLD Version\n",
        "        # (16, 12, 12, 7)\n",
        "        # Decompose input data\n",
        "        # Initially in is a mix of Current State and e_char embedding space\n",
        "        # --------------------------------------------------------------\n",
        "        # input_current_state = inputs[..., 0:12, 0:12, 0:6]\n",
        "        # e_char = inputs[..., 12, 0, :]\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 12, 12, 6) -> (16, 12, 12, 32)\n",
        "        # (16, 12, 12, 7) -> (16, 12, 12, 32)\n",
        "        # Use 3x3 conv layer to shape the depth to 32\n",
        "        # to enable resnet to work (addition between main path and residual connection)\n",
        "        # --------------------------------------------------------------\n",
        "        # x = self.conv_1(input_current_state)\n",
        "        x = self.conv_1(inputs)\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 12, 12, 32) -> (16, 12, 12, 32)\n",
        "        # Add n residual layers\n",
        "        # Conv2D takes only width x height x channels (12, 12, 11)\n",
        "        # Time Distributed layer feeds a Conv2D with time-frames (10 frames)\n",
        "        # That process is happening in parallel for 16 objects in one batch\n",
        "        # --------------------------------------------------------------\n",
        "        for i in range(n):\n",
        "          x = self.res_blocks[i](x)    ### Possible error here!!!\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 12, 12, 32) -> (16, 12, 12, 32)\n",
        "        # Add CNN after Res Blocks\n",
        "        # --------------------------------------------------------------\n",
        "        x = self.conv_2(x)\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 12, 12, 32) -> (16, 32)\n",
        "        # Add average pooling\n",
        "        # Collapse the spatial dimensions\n",
        "        # --------------------------------------------------------------\n",
        "        x = tf.reduce_mean(input_tensor=x, axis=[1, 2])\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 32) + (16, 8) -> (16, 32, 1) + (16, 8, 1) - >\n",
        "        # (16, 40, 1) -> (16, 40)\n",
        "        # Concatenate tensor with e_char\n",
        "        # Concatenation requires a common dimentions which cannot be a batch\n",
        "        # --------------------------------------------------------------\n",
        "        # x = tf.expand_dims(x, axis=-1)\n",
        "        # e_char = tf.expand_dims(e_char, axis=-1)\n",
        "        #\n",
        "        # x = tf.keras.layers.Concatenate(axis=1)([x, e_char])\n",
        "        # x = x[..., 0]\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 40) -> (16, 60) -> (16, 4)\n",
        "        # Fully connected layer with dropout for regularization\n",
        "        # --------------------------------------------------------------\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x =  self.goal_predict(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "5R_FPRwC_W09"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ToMnet-N"
      ],
      "metadata": {
        "id": "jxaTaJfs_xnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ToMnet\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LSTM\n",
        "from keras import activations\n",
        "\n",
        "class ToMnet(Model):\n",
        "\n",
        "    LENGTH_E_CHAR = 8\n",
        "    NUM_RESIDUAL_BLOCKS = 8\n",
        "\n",
        "    def __init__(self, ts, w, h, d, Ne_char=8, N_res_blocks=8, filters=32):\n",
        "        super(ToMnet, self).__init__(name=\"ToMnet-N\")\n",
        "\n",
        "        self.MAX_TRAJECTORY_SIZE = ts  # 20-50\n",
        "        self.MAZE_WIDTH = w  # 12\n",
        "        self.MAZE_HEIGHT = h  # 12\n",
        "        self.MAZE_DEPTH_TRAJECTORY = d  # 10\n",
        "        self.LENGTH_E_CHAR = Ne_char\n",
        "        self.NUM_RESIDUAL_BLOCKS = N_res_blocks\n",
        "\n",
        "        self.INPUT_SHAPE = (self.MAX_TRAJECTORY_SIZE+1, self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH_TRAJECTORY)\n",
        "        self.TRAJECTORY_SHAPE = (self.MAX_TRAJECTORY_SIZE, self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH_TRAJECTORY) # 20x12x12x10\n",
        "        self.CURRENT_STATE_SHAPE = (self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH_TRAJECTORY-4)                      # 12x12x6\n",
        "\n",
        "        # Create the model\n",
        "        self.char_net = CharNet(input_tensor=self.TRAJECTORY_SHAPE,\n",
        "                                n=self.NUM_RESIDUAL_BLOCKS,\n",
        "                                N_echar=self.LENGTH_E_CHAR,\n",
        "                                filters=filters)\n",
        "\n",
        "        self.pred_net = PredNet(n=self.NUM_RESIDUAL_BLOCKS, filters=filters)\n",
        "\n",
        "        # Set compilers / savers / loggers / callbacks\n",
        "\n",
        "    def call(self, data):\n",
        "\n",
        "        # To fix ERROR with Tensor <-> Numpy compatibility\n",
        "        tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "        input_trajectory = data[0]   # input_traj            # inputs[..., 0:self.MAX_TRAJECTORY_SIZE, :, :, :]\n",
        "        input_current_state = data[1] #  input_current    # inputs[..., self.MAX_TRAJECTORY_SIZE, :, :, 0:6]\n",
        "\n",
        "        e_char = self.char_net(input_trajectory)\n",
        "\n",
        "        print(\"In ToMnet-N: \")\n",
        "        print(\"input_trajectory: \", input_trajectory.shape)\n",
        "        print(\"input_current_state: \", input_current_state.shape)\n",
        "        print(\"e_char SHAPE: \", e_char.shape)\n",
        "        print(\"e_char TYPE\", type(e_char))\n",
        "        print(\"e_char\", e_char)\n",
        "\n",
        "        # --------------------------------------------------------------\n",
        "        # Paper codes\n",
        "        # (16, 12, 12, 6) + (16, 8) ->\n",
        "        # (16, 12, 12, 6) + (16, 8+4zero, 12repeat, 1) ->\n",
        "        # (16, 12, 12, 7)   # NEW VERSION\n",
        "        # (16, 12, 12, 8) + (16, 1, 12, 8) -> (16, 13, 12, 8)   # OLD VERSION\n",
        "        # Spatialise and unite different data into one tensor\n",
        "        # They are automatically decompose in the Pred Net to different data\n",
        "        # --------------------------------------------------------------\n",
        "        e_char_new = tf.concat(values=[e_char,e_char], axis = 1) # tf.repeat(e_char, repeats=2, axis=-1)\n",
        "        e_char_new = e_char_new[..., 0:12]\n",
        "        print(\"Before Concatenation: \", e_char.shape)\n",
        "        print(\"After Concatenation: \", e_char_new.shape)\n",
        "\n",
        "        # print(\"e_char_new: \", e_char_new.shape)\n",
        "        e_char_new = tf.expand_dims(e_char_new, axis=-1)\n",
        "        # print(\"e_char_new: \", e_char_new.shape)\n",
        "        e_char_new = tf.repeat(e_char_new, repeats=12, axis=-1)\n",
        "        # print(\"e_char_new: \", e_char_new.shape)\n",
        "        e_char_new = tf.expand_dims(e_char_new, axis=-1)\n",
        "        print(\"e_char_new: \", e_char_new.shape)\n",
        "        print(\"input_current_state: \", input_current_state.shape)\n",
        "        input_current_state = tf.cast(input_current_state, tf.float32)\n",
        "\n",
        "        mix_data = tf.keras.layers.Concatenate(axis=-1)([input_current_state, e_char_new])\n",
        "\n",
        "        print(\"mix_data (pred input): \", mix_data.shape)\n",
        "\n",
        "        pred = self.pred_net(mix_data)\n",
        "        output = pred\n",
        "        return output\n",
        "\n",
        "    ### This is a trick to view shapes in summary() via\n",
        "    ### model.model().summary()\n",
        "    def model(self):\n",
        "        x = tf.keras.Input(shape=self.INPUT_SHAPE)\n",
        "        return tf.keras.Model(inputs=[x], outputs=self.call(x))"
      ],
      "metadata": {
        "id": "VYLZCpus_ceY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "LdBF7API_uFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Loader\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "class DataHandler(mp.ModelParameter):\n",
        "\n",
        "The class for parsing txt data.\n",
        "\n",
        "Note:\n",
        "  Inherit mp.ModelParameter to share model constants.\n",
        "\n",
        "@author: Chuang, Yun-Shiuan; Edwinn\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "The data stored like: 1x12x12x10. 1 - Time Step, 12x12 - Map Resolution, 10 - Depth (1 walls, 1 player, 4 goals, 4 actions)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import re\n",
        "\n",
        "class DataHandler:\n",
        "\n",
        "    def __init__(self, ts, w, h, d):\n",
        "        self.MAX_TRAJECTORY_SIZE = ts # 20-50\n",
        "        self.MAZE_WIDTH = w # 12\n",
        "        self.MAZE_HEIGHT = h # 12\n",
        "        self.MAZE_DEPTH_TRAJECTORY = d # 11\n",
        "\n",
        "        # Constants to keep track on standsrd\n",
        "        # At which games are saved\n",
        "        self.MAZE_LINE_START = 2\n",
        "        self.MAZE_LINE_END = self.MAZE_WIDTH + 2\n",
        "        self.CONSUMED_GOAL = self.MAZE_LINE_END + 1\n",
        "        self.TRAJ_LENGTH = self.CONSUMED_GOAL + 1\n",
        "        self.TRAJ_START = self.TRAJ_LENGTH + 1\n",
        "\n",
        "    # It loads full trajectory, sequence of actions and consumed goal per game\n",
        "    def load_all_games(self, directory, use_percentage=1):\n",
        "\n",
        "        # Get names of games\n",
        "        files = os.listdir(directory)\n",
        "        r = re.compile(\".*.txt\")\n",
        "        files = list(filter(r.match, files))\n",
        "        Nfiles = len(files)\n",
        "        Nfraction = int(np.ceil(use_percentage * Nfiles))   # Apply a fraction division\n",
        "        files = files[:Nfraction]\n",
        "        print(\"----\")\n",
        "        print(\"Saved Games found: \", Nfiles)\n",
        "        print(\"Saved Games loaded: \", Nfraction)\n",
        "        print(\"Percentage of loaded games: \", use_percentage*100, \"%\")\n",
        "        print(\"Games names: \", files)\n",
        "\n",
        "        # Save all trajectories and labels\n",
        "        trajectories = [] # np.empty([1, self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH_TRAJECTORY])\n",
        "        actions = [] # np.empty(1)\n",
        "        labels = [] # np.empty(1)\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Parse file one by one\n",
        "        # ------------------------------------------------------------------\n",
        "        j = 0  # for tracking progress (%)\n",
        "        for i, file in enumerate(files):\n",
        "\n",
        "            # Read one game\n",
        "            traj, act, goal = self.read_one_game(filename=os.path.join(directory, file))\n",
        "\n",
        "            # Append a game to data\n",
        "            trajectories.append(traj)\n",
        "            actions.append(act)\n",
        "            labels.append(goal)\n",
        "\n",
        "            # Keep track on progress\n",
        "            if i >= int(np.ceil(j * Nfraction / 100))-1:\n",
        "                print('Parsed ' + str(j) + '%')\n",
        "                j += 10\n",
        "        print(\"----\")\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Prepare data from games. -> Make many trajectories for each game\n",
        "        # ------------------------------------------------------------------\n",
        "        print(\"Augment data. One game creates many training samples!\")\n",
        "\n",
        "        data_trajectories = []\n",
        "        data_current_state = []\n",
        "        data_actions = []\n",
        "        data_labels = []\n",
        "        j = 0  # for tracking progress (%)\n",
        "\n",
        "        # Process Game-per-Game\n",
        "        for i in range(Nfraction):\n",
        "\n",
        "            # Consider only games with more than 6 moves\n",
        "            if trajectories[i].shape[0] < 6:\n",
        "                continue\n",
        "\n",
        "            # Prepare data from one game\n",
        "            data_trajectories1, data_current_state1, data_actions1, data_labels1 = self.generate_data_from_game(trajectories=trajectories[i],                                                                                                 actions=actions[i],\n",
        "                                                                                                                labels=labels[i])\n",
        "            # Append to a single structure\n",
        "            data_trajectories.append(data_trajectories1)\n",
        "            data_current_state.append(data_current_state1)\n",
        "            data_actions.append(data_actions1)\n",
        "            data_labels.append(data_labels1)\n",
        "\n",
        "            # Keep track on progress\n",
        "            if i >= int(np.ceil(j * Nfraction / 100))-1:\n",
        "                print('Augmented data ' + str(j) + '%')\n",
        "                j += 10\n",
        "\n",
        "        print(\"----\")\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # Split the data  to Train / Test / Valid\n",
        "        # ------------------------------------------------------------------\n",
        "        print(\"Create training/testing/validation sets\")\n",
        "\n",
        "        train_traj, test_traj, valid_traj, \\\n",
        "        train_current, test_current, valid_current, \\\n",
        "        train_goal, test_goal, valid_goal, \\\n",
        "        train_act, test_act, valid_act = self.split_and_shaffle(data_trajectories=data_trajectories,\n",
        "                                                                  data_current_state=data_current_state,\n",
        "                                                                  data_actions=data_actions,\n",
        "                                                                  data_labels=data_labels)\n",
        "\n",
        "        print(\"----\")\n",
        "\n",
        "        return train_traj, test_traj, valid_traj, \\\n",
        "               train_current, test_current, valid_current, \\\n",
        "               train_goal, test_goal, valid_goal, \\\n",
        "               train_act,  test_act,  valid_act\n",
        "\n",
        "    def load_all_games_v2(self, directory, use_percentage = 1.0):\n",
        "\n",
        "        # Get names of games\n",
        "        files = os.listdir(directory)\n",
        "        r = re.compile(\".*.txt\")\n",
        "        files = list(filter(r.match, files))\n",
        "        Nfiles = len(files)\n",
        "        Nfraction = int(np.ceil(use_percentage * Nfiles))  # Apply a fraction division\n",
        "        files = files[:Nfraction]\n",
        "        print(\"----\")\n",
        "        print(\"Saved Games found: \", Nfiles)\n",
        "        print(\"Saved Games loaded: \", Nfraction)\n",
        "        print(\"Percentage of loaded games: \", use_percentage * 100, \"%\")\n",
        "        print(\"Games names: \", files)\n",
        "\n",
        "        # Save all trajectories and labels\n",
        "        trajectories = []  # np.empty([1, self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH_TRAJECTORY])\n",
        "        actions = []  # np.empty(1)\n",
        "        labels = []  # np.empty(1)\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # 1. Load each game one by one\n",
        "        # ------------------------------------------------------------------\n",
        "        j = 0  # for tracking progress (%)\n",
        "        for i, file in enumerate(files):\n",
        "\n",
        "            # Read one game\n",
        "            traj, act, goal = self.read_one_game(filename=os.path.join(directory, file))\n",
        "\n",
        "            # Append a game to data\n",
        "            trajectories.append(traj)\n",
        "            actions.append(act)\n",
        "            labels.append(goal)\n",
        "\n",
        "            # Keep track on progress\n",
        "            if i >= int(np.ceil(j * Nfraction / 100)) - 1:\n",
        "                print('Parsed ' + str(j) + '%')\n",
        "                j += 10\n",
        "        print(\"----\")\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # 2. Make many Trajectory-Current state pairs from all  games\n",
        "        # ------------------------------------------------------------------\n",
        "        print(\"Augment data. One game creates many training samples!\")\n",
        "\n",
        "        data_trajectories = []\n",
        "        data_current_state = []\n",
        "        data_actions = []\n",
        "        data_labels = []\n",
        "        j = 0  # for tracking progress (%)\n",
        "\n",
        "        # Process Game-per-Game\n",
        "        for i in range(Nfraction):\n",
        "\n",
        "            # Consider only games with more than 6 moves\n",
        "            if trajectories[i].shape[0] < 6:\n",
        "                continue\n",
        "\n",
        "            # Prepare data from one game\n",
        "            # The dimensions differ, so only list is applicable (no numpy arrays)\n",
        "            data_trajectories1, data_current_state1, \\\n",
        "            data_actions1, data_labels1 = self.generate_data_from_game(\n",
        "                trajectories=trajectories[i],\n",
        "                actions=actions[i],\n",
        "                labels=labels[i])\n",
        "\n",
        "            # Append to a single structure\n",
        "            data_trajectories.append(data_trajectories1)\n",
        "            data_current_state.append(data_current_state1)\n",
        "            data_actions.append(data_actions1)\n",
        "            data_labels.append(data_labels1)\n",
        "\n",
        "            # Keep track on progress\n",
        "            if i >= int(np.ceil(j * Nfraction / 100)) - 1:\n",
        "                print('Augmented data ' + str(j) + '%')\n",
        "                j += 10\n",
        "\n",
        "        print(\"----\")\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # 2. Putting all data together\n",
        "        # ------------------------------------------------------------------\n",
        "\n",
        "        # data_trajectories1 shape is ()\n",
        "        all_games = {\n",
        "            \"traj_history\": data_trajectories,\n",
        "            \"current_state_history\": data_current_state,\n",
        "            \"actions_history\": data_actions\n",
        "        }\n",
        "\n",
        "        return all_games\n",
        "\n",
        "\n",
        "    def load_one_game(self, directory):\n",
        "\n",
        "        files = os.listdir(directory)\n",
        "        r = re.compile(\".*.txt\")\n",
        "        files = list(filter(r.match, files))\n",
        "\n",
        "        # Load the game with min 10 steps\n",
        "        traj = []\n",
        "        act = []\n",
        "        goal = []\n",
        "        game_length = 0\n",
        "        while game_length < 10:\n",
        "            one_game = random.choice(files)\n",
        "            filename = os.path.join(directory, one_game)\n",
        "            traj, act, goal = self.read_one_game(filename)\n",
        "            game_length = traj.shape[0]\n",
        "\n",
        "        traj_history, current_state_history, actions_history, _ = self.generate_data_from_game(traj, act, goal)\n",
        "\n",
        "        single_game = {\n",
        "            \"traj\": traj,   # Original trajectory\n",
        "            \"act\": act,\n",
        "            \"goal\": goal,\n",
        "            \"ToM\":\n",
        "                {\n",
        "                    \"traj_history\": traj_history,   # Sequence of trajectories for ToMnet predictions. NO ZERO PADDING HERE!\n",
        "                    \"current_state_history\": current_state_history,\n",
        "                    \"actions_history\": actions_history\n",
        "                }\n",
        "        }\n",
        "\n",
        "        return single_game\n",
        "\n",
        "    # Returns trajectory, actions and consumed goal\n",
        "    # For a single game\n",
        "    def read_one_game(self, filename):\n",
        "        '''\n",
        "            Return\n",
        "                traj - (ActionsInGame x MapWidth x MapHeight x MapDepth) (15x12x12x10)\n",
        "                actions - (ActionsInGame) (array of numbers representing actions)\n",
        "                goal - (ActionsInGame)  (array of the same goal *For Experiment 1*)\n",
        "        '''\n",
        "\n",
        "        traj = np.empty((1, self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH_TRAJECTORY))\n",
        "        act  = np.empty(1, dtype=np.int8)\n",
        "        goal = np.empty(1, dtype=np.int8)\n",
        "\n",
        "        # output.shape(100, 12, 12, 10) where 100 is Max Trajectory Size, 12x12 is WidthxHeight and 10 is Depth (1walls + 1player + 4goals + 4actions)\n",
        "        output = np.zeros((self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH_TRAJECTORY, self.MAX_TRAJECTORY_SIZE))\n",
        "        label = ''\n",
        "        steps = []\n",
        "        with open(filename) as fp:\n",
        "            lines = list(fp)\n",
        "            maze = lines[self.MAZE_LINE_START:self.MAZE_LINE_END]\n",
        "\n",
        "            # Parse maze to 2d array, remove boundary walls.\n",
        "            for i in range(self.MAZE_WIDTH):\n",
        "                maze[i] = list(maze[i])\n",
        "                maze[i] = maze[i][1:len(maze[i]) - 2]   # Transform: #row#\\n -> row\n",
        "\n",
        "            # Original maze (without walls)\n",
        "            np_maze = np.array(maze)\n",
        "\n",
        "            # Plane for obstacles\n",
        "            np_obstacles = np.where(np_maze == '#', 1, 0).astype(np.int8)\n",
        "\n",
        "            # Plane for agent's initial position\n",
        "            np_agent = np.where(np_maze == 'O', 1, 0).astype(np.int8)\n",
        "\n",
        "            # Plane for goals\n",
        "            targets = ['A', 'B', 'C', 'D']  # for the simplified 4-targets mazes\n",
        "            np_targets = np.repeat(np_maze[:, :, np.newaxis], len(targets), axis=2)\n",
        "            for target, i in zip(targets, range(len(targets))):\n",
        "                np_targets[:, :, i] = np.where(np_maze == target, 1, 0)\n",
        "            np_targets = np_targets.astype(int)\n",
        "\n",
        "            # Save Consumed Goal\n",
        "            goal_line = lines[self.CONSUMED_GOAL]\n",
        "            _, goal_sym = goal_line.split(\" : \")\n",
        "            goal_sym = goal_sym[0]\n",
        "            goal_num = self.goal_sym_to_num(goal_sym)\n",
        "\n",
        "            # Get Trajectory Length\n",
        "            Ntraj_line = lines[self.TRAJ_LENGTH]\n",
        "            _, Ntraj = Ntraj_line.split(\": \")\n",
        "            Ntraj = int(Ntraj)\n",
        "\n",
        "            # Save Actions & Save Trajectory\n",
        "            trajectory = lines[self.TRAJ_START : self.TRAJ_START + Ntraj]\n",
        "            agent_locations = []\n",
        "            for i, tau in enumerate(trajectory):\n",
        "                # Decompose\n",
        "                tau = tau[:len(tau) - 1]  # Transform: 'output\\n' -> 'output'\n",
        "                tmp = tau.split(\" : \")\n",
        "                pos = tmp[0]\n",
        "                pos = pos[1:-1]\n",
        "                row, col = pos.split(\", \")\n",
        "\n",
        "                # Save\n",
        "                # NOTE: first element in act & goal are trash values and MUST be replaced\n",
        "                if i == 0:\n",
        "                    agent_locations.append([int(row), int(col)])\n",
        "                    act[0] = int(tmp[1])\n",
        "                    goal[0] = goal_num  # self.sym_to_goal(tmp[2], consumed=)\n",
        "                else:\n",
        "                    agent_locations.append([int(row), int(col)])\n",
        "                    act = np.append(act, int(tmp[1]))\n",
        "                    goal = np.append(goal, goal_num) # self.sym_to_goal(tmp[2], consumed=)\n",
        "\n",
        "                    # Update Agent Location Tensor\n",
        "                    np_agent = np.zeros(shape=(self.MAZE_WIDTH, self.MAZE_HEIGHT), dtype = np.int8)\n",
        "                    np_agent[int(row), int(col)] = 1\n",
        "\n",
        "                # Make Trajectory Tensor\n",
        "                np_actions = np.zeros((self.MAZE_WIDTH, self.MAZE_HEIGHT, 4), dtype=np.int8)\n",
        "                a = act[i]\n",
        "                np_actions[int(row), int(col), a] = 1\n",
        "\n",
        "                np_tensor = np.dstack((np_obstacles, np_agent, np_targets, np_actions)) # (1walls + 1player + 4goals + 4actions)\n",
        "                steps.append(np_tensor)\n",
        "                traj = np.array(steps)\n",
        "\n",
        "        fp.close()\n",
        "        return traj, act, goal\n",
        "\n",
        "    def goal_sym_to_num(self, goal_sym):\n",
        "        out = 0\n",
        "        if goal_sym == \"A\":\n",
        "            out = 1\n",
        "        elif goal_sym == \"B\":\n",
        "            out = 2\n",
        "        elif goal_sym == \"C\":\n",
        "            out = 3\n",
        "        elif goal_sym == \"D\":\n",
        "            out = 4\n",
        "        else:\n",
        "            raise ValueError(\"ERROR: wrong goal sym was given!\")\n",
        "        return out\n",
        "\n",
        "    # It deconstructs each game to a series of samples.\n",
        "    # Single trajectory becomes a sequence of rising trajectories with same\n",
        "    # Consumed goals\n",
        "    def generate_data_from_game(self, trajectories, actions, labels):\n",
        "\n",
        "        # Make full data from a game\n",
        "        data_trajectories = []\n",
        "        data_current_state = []\n",
        "        data_actions = []\n",
        "        data_labels = []\n",
        "\n",
        "        MIN_ACTIONS = 5\n",
        "        for i in range(MIN_ACTIONS, trajectories.shape[0]):\n",
        "            data_trajectories.append(trajectories[0:i,...])     # Trajectory to the state\n",
        "            data_current_state.append(trajectories[i,..., 0:6]) # Current state # (1walls + 1player + 4goals)\n",
        "            data_actions.append(actions[i,...])                 # Next Action\n",
        "            data_labels.append(labels[i,...])                   # Consumed Goal\n",
        "\n",
        "        return data_trajectories, data_current_state, data_actions, data_labels\n",
        "\n",
        "    def split_and_shaffle(self, data_trajectories, data_current_state, data_actions, data_labels):\n",
        "\n",
        "        N_Total = len(data_trajectories)\n",
        "        N_train = int(np.ceil(N_Total * 0.65))\n",
        "        N_test  = int(np.ceil(N_Total * 0.20))\n",
        "        N_valid = int(np.ceil(N_Total * 0.15))\n",
        "\n",
        "        print(\"Total number of games after filtration: \", N_Total)\n",
        "        print(\"Games for training: \", N_train)\n",
        "        print(\"Games for testing: \", N_test)\n",
        "        print(\"Games for validation: \", N_valid)\n",
        "\n",
        "        total_indexes = list(range(N_Total))\n",
        "        shuffle(total_indexes)\n",
        "\n",
        "        train_indexes = total_indexes[0:N_train]\n",
        "        test_indexes  = total_indexes[N_train:N_train+N_test]\n",
        "        valid_indexes = total_indexes[N_train+N_test:]\n",
        "\n",
        "        train_traj = [data_trajectories[i] for i in train_indexes]\n",
        "        test_traj = [data_trajectories[i] for i in test_indexes]\n",
        "        valid_traj = [data_trajectories[i] for i in valid_indexes]\n",
        "\n",
        "        train_current = [data_current_state[i] for i in train_indexes]\n",
        "        test_current = [data_current_state[i] for i in test_indexes]\n",
        "        valid_current = [data_current_state[i] for i in valid_indexes]\n",
        "\n",
        "        train_goal = [data_labels[i] for i in train_indexes]\n",
        "        test_goal = [data_labels[i] for i in test_indexes]\n",
        "        valid_goal = [data_labels[i] for i in valid_indexes]\n",
        "\n",
        "        train_act = [data_actions[i] for i in train_indexes]\n",
        "        test_act = [data_actions[i] for i in test_indexes]\n",
        "        valid_act = [data_actions[i] for i in valid_indexes]\n",
        "\n",
        "        # Unpack lists in data to be a single list of all games\n",
        "        train_traj = sum(train_traj, [])\n",
        "        test_traj = sum(test_traj, [])\n",
        "        valid_traj = sum(valid_traj, [])\n",
        "\n",
        "        train_current = sum(train_current, [])\n",
        "        test_current = sum(test_current, [])\n",
        "        valid_current = sum(valid_current, [])\n",
        "\n",
        "        train_goal = sum(train_goal, [])\n",
        "        test_goal = sum(test_goal, [])\n",
        "        valid_goal = sum(valid_goal, [])\n",
        "\n",
        "        train_act = sum(train_act, [])\n",
        "        test_act = sum(test_act, [])\n",
        "        valid_act = sum(valid_act, [])\n",
        "\n",
        "        print(\"Time Steps for training: \", len(train_act))\n",
        "        print(\"Time Steps for testing: \", len(test_act))\n",
        "        print(\"Time Steps for validation: \", len(valid_act))\n",
        "\n",
        "        return train_traj, test_traj, valid_traj, \\\n",
        "               train_current, test_current, valid_current, \\\n",
        "               train_goal, test_goal, valid_goal, \\\n",
        "               train_act, test_act, valid_act"
      ],
      "metadata": {
        "id": "Z0kGnNDs_guJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "kovClPHw_9Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Data Processing\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "class DataHandler(mp.ModelParameter):\n",
        "\n",
        "The class for parsing txt data.\n",
        "\n",
        "Note:\n",
        "  Inherit mp.ModelParameter to share model constants.\n",
        "\n",
        "@author: Chuang, Yun-Shiuan; Edwinn\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "The data stored like: 1x12x12x10. 1 - Time Step, 12x12 - Map Resolution, 10 - Depth (1 walls, 1 player, 4 goals, 4 actions)\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.transforms as mtransforms\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import re\n",
        "\n",
        "class DataProcessor:\n",
        "\n",
        "    def __init__(self, ts, w, h, d):\n",
        "        self.MAX_TRAJECTORY_SIZE = ts # 20-50\n",
        "        self.MAZE_WIDTH = w # 12\n",
        "        self.MAZE_HEIGHT = h # 12\n",
        "        self.MAZE_DEPTH = d # 10 (1player + 1wall + 4goals + 4 actions = 10)\n",
        "\n",
        "        # Constants to keep track on standsrd\n",
        "        # At which games are saved\n",
        "        self.MAZE_LINE_START = 2\n",
        "        self.MAZE_LINE_END = self.MAZE_WIDTH + 2\n",
        "        self.CONSUMED_GOAL = self.MAZE_LINE_END + 1\n",
        "        self.TRAJ_LENGTH = self.CONSUMED_GOAL + 1\n",
        "        self.TRAJ_START = self.TRAJ_LENGTH + 1\n",
        "\n",
        "    def zero_pad_single_game(self, max_elements, single_game):\n",
        "\n",
        "        # A single game has several trajectories\n",
        "        all_trajectories = single_game[\"ToM\"][\"traj_history\"]\n",
        "        N = len(all_trajectories)\n",
        "        TrajZeroPad = []\n",
        "\n",
        "        for i in range(N):\n",
        "            zero_pad_trajectory = np.zeros(shape=(max_elements,\n",
        "                                        self.MAZE_WIDTH,\n",
        "                                        self.MAZE_HEIGHT,\n",
        "                                        self.MAZE_DEPTH))\n",
        "            current_trajectory = all_trajectories[i]\n",
        "            Nt = len(current_trajectory)  # Number of real steps in the current trajectory\n",
        "            if Nt > max_elements:\n",
        "                zero_pad_trajectory = current_trajectory[-max_elements:]\n",
        "            else:\n",
        "                zero_pad_trajectory[:Nt, ...] = current_trajectory\n",
        "            TrajZeroPad.append(zero_pad_trajectory)\n",
        "\n",
        "        single_game[\"ToM\"][\"traj_history_zp\"] = TrajZeroPad\n",
        "\n",
        "        return single_game\n",
        "\n",
        "\n",
        "    # It adds zeros at the beginning of the trajectories\n",
        "    def zero_padding(self, max_elements, DictData):\n",
        "\n",
        "        DataZeroPad = DictData.copy()\n",
        "\n",
        "        for key, value in DictData.items():\n",
        "\n",
        "            if key[-len(\"traj\"):] == \"traj\":\n",
        "                print(\"Apply Zero-Padding to \" + key + \"... \")\n",
        "                all_trajectories = DictData[key]\n",
        "                N = len(all_trajectories)\n",
        "                TrajZeroPad = []\n",
        "\n",
        "                # Fill the last elements with real trajectory (implement pre-zero padding)\n",
        "                for i in range(N):\n",
        "                    zero_pad_trajectory = np.zeros(shape=(max_elements,\n",
        "                                                          self.MAZE_WIDTH,\n",
        "                                                          self.MAZE_HEIGHT,\n",
        "                                                          self.MAZE_DEPTH))\n",
        "                    current_trajectory = all_trajectories[i]\n",
        "                    Nt =  len(current_trajectory) # Number of real steps in the trajectory\n",
        "                    if Nt > max_elements:\n",
        "                        zero_pad_trajectory = current_trajectory[-max_elements:]\n",
        "                    else:\n",
        "                        zero_pad_trajectory[:Nt, ...] = current_trajectory\n",
        "\n",
        "                    # if key == \"valid_traj\":\n",
        "                    #     actions = DictData[\"valid_act\"]\n",
        "                    #     ac = actions[i] # Getting an action TOMnet must predict\n",
        "                    #     print(\"Next Action should be: \", ac)\n",
        "                    #     self.one_trajectory_validation(zero_pad_trajectory)\n",
        "                    #     cur_states = DictData[\"valid_current\"]\n",
        "                    #     cur_state = cur_states[i]\n",
        "                    #     self.current_validation(cur_state)\n",
        "                    TrajZeroPad.append(zero_pad_trajectory)\n",
        "\n",
        "                DataZeroPad[key] = TrajZeroPad\n",
        "\n",
        "        print(\"Zero Padding was applied!\")\n",
        "\n",
        "        return DataZeroPad\n",
        "\n",
        "        # It adds zeros at the beginning of the trajectories\n",
        "    def zero_padding_v2(self, max_elements, all_games):\n",
        "\n",
        "        # all_games = {\n",
        "        #     \"traj_history\": traj_history,\n",
        "        #     \"traj_history_zp\": traj_history_zp                    # Trajectory with Zero Padding\n",
        "        #     \"current_state_history\": current_state_history,\n",
        "        #     \"actions_history\": actions_history\n",
        "        # }\n",
        "\n",
        "        uniform_shape = (1, max_elements, self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH)\n",
        "\n",
        "        zero_padded_trajectories = []   # ndarray, not list\n",
        "        unfolded_current_states = []\n",
        "        unfolded_action_history = []\n",
        "        all_trajectories = all_games[\"traj_history\"]\n",
        "        all_current_states = all_games[\"current_state_history\"]\n",
        "        all_actions = all_games[\"actions_history\"]\n",
        "        N_all_games = len(all_trajectories)\n",
        "\n",
        "        # Go one by one game\n",
        "        # Where each game consist of many trajectories\n",
        "        tracker_var = 0\n",
        "        for i in range(N_all_games):\n",
        "\n",
        "            traj = all_trajectories[i]\n",
        "            cur = all_current_states[i]\n",
        "            act = all_actions[i]\n",
        "            N_traj = len(traj) # traj.shape[0]      # Number of trajectories in current game\n",
        "\n",
        "            for j in range(N_traj):\n",
        "\n",
        "                ### Init single piece of data from a game\n",
        "                current_trajectory = traj[j]\n",
        "                current_state = cur[j]\n",
        "                current_action = act[j]\n",
        "\n",
        "                ### Trajectory\n",
        "                zero_pad_trajectory = np.zeros(shape=uniform_shape)\n",
        "                Nt = current_trajectory.shape[0]  # Number of real steps in the trajectory\n",
        "\n",
        "                # Save game in a bigger array so the rest is fiiled with zeros\n",
        "                if Nt > max_elements:\n",
        "                    zero_pad_trajectory[0, ...] = current_trajectory[-max_elements:]\n",
        "                else:\n",
        "                    zero_pad_trajectory[0, 0:Nt, ...] = current_trajectory\n",
        "\n",
        "                zero_padded_trajectories.append(zero_pad_trajectory[0,...])\n",
        "\n",
        "                ### Current state\n",
        "                unfolded_current_states.append(current_state)\n",
        "\n",
        "                ### Action\n",
        "                unfolded_action_history.append(current_action)\n",
        "\n",
        "\n",
        "            # Keep track on progress\n",
        "            if i >= int(N_all_games * tracker_var / 100) - 2:\n",
        "                print('Zero-Padded data ' + str(tracker_var) + '%')\n",
        "                tracker_var += 5\n",
        "\n",
        "        zero_padded_trajectories = np.array(zero_padded_trajectories)\n",
        "        unfolded_current_states = np.array(unfolded_current_states)\n",
        "        unfolded_action_history = np.array(unfolded_action_history)\n",
        "        all_games[\"traj_history_zp\"] = zero_padded_trajectories\n",
        "        all_games[\"current_state_history\"] = unfolded_current_states\n",
        "        all_games[\"actions_history\"] = unfolded_action_history\n",
        "\n",
        "        print(all_games[\"traj_history_zp\"].shape)\n",
        "\n",
        "        print(\"Zero Padding was applied!\")\n",
        "\n",
        "        return all_games\n",
        "\n",
        "    # Traj: 20x12x12x10\n",
        "    # Cur: 12x12x6\n",
        "    # ToMnet input: 21x12x12x10\n",
        "    # Cur: 12x12x6 -> 1x12x12x10\n",
        "    # Concatenate 20x12x12x10 + 1x12x12x10 -> 21x12x12x10\n",
        "    def unite_traj_current(self, DictData):\n",
        "\n",
        "        UniData = {\n",
        "            \"train_input\": [np.zeros(shape=(self.MAX_TRAJECTORY_SIZE+1,\n",
        "                                     self.MAZE_WIDTH,\n",
        "                                     self.MAZE_HEIGHT,\n",
        "                                     self.MAZE_DEPTH))] * len(DictData[\"train_traj\"]),\n",
        "            \"test_input\": [np.zeros(shape=(self.MAX_TRAJECTORY_SIZE + 1,\n",
        "                                     self.MAZE_WIDTH,\n",
        "                                     self.MAZE_HEIGHT,\n",
        "                                     self.MAZE_DEPTH))] * len(DictData[\"test_traj\"]),\n",
        "            \"valid_input\": [np.zeros(shape=(self.MAX_TRAJECTORY_SIZE + 1,\n",
        "                                    self.MAZE_WIDTH,\n",
        "                                    self.MAZE_HEIGHT,\n",
        "                                    self.MAZE_DEPTH))] * len(DictData[\"valid_traj\"]),\n",
        "            \"train_goal\": DictData[\"train_goal\"],\n",
        "            \"test_goal\": DictData[\"test_goal\"],\n",
        "            \"valid_goal\": DictData[\"valid_goal\"],\n",
        "            \"train_act\": DictData[\"train_act\"],\n",
        "            \"test_act\": DictData[\"test_act\"],\n",
        "            \"valid_act\": DictData[\"valid_act\"]\n",
        "        }\n",
        "\n",
        "        print(\"-----\")\n",
        "        for key, value in UniData.items():\n",
        "\n",
        "            if key[-len(\"input\"):] == \"input\":\n",
        "\n",
        "                print(\"Apply concatenation to \" + key + \"... \")\n",
        "                # Add Trajectory in the beginning\n",
        "                purpose = key[:-(len(\"input\")+1)] # train / test / valid\n",
        "                for i in range(len(DictData[purpose + \"_traj\"])):\n",
        "                    UniData[key][i][0:self.MAX_TRAJECTORY_SIZE] = DictData[purpose + \"_traj\"][i]\n",
        "\n",
        "                # Add Current in the end\n",
        "                for i in range(len(DictData[purpose + \"_traj\"])):\n",
        "                    # 12x12x6 -> 12x12x10\n",
        "                    data_expanded = np.repeat(DictData[purpose + \"_current\"][i], repeats=2, axis=-1)\n",
        "                    data_expanded = data_expanded[..., 0:10]\n",
        "                    UniData[key][i][self.MAX_TRAJECTORY_SIZE] = data_expanded\n",
        "\n",
        "        print(\"Concatenation is finished\")\n",
        "        return UniData\n",
        "\n",
        "\n",
        "    def unite_single_traj_current(self, single_game):\n",
        "\n",
        "        print(\"Apply concatenation to a single trajectory... \")\n",
        "        trajectories_list = single_game[\"ToM\"][\"traj_history\"]\n",
        "        current_state_list = single_game[\"ToM\"][\"current_state_history\"]\n",
        "        assert len(trajectories_list) == len(current_state_list)\n",
        "\n",
        "        N = len(trajectories_list)\n",
        "        united_data = []\n",
        "        for i in range(N):\n",
        "\n",
        "            cur_traj = trajectories_list[i]     # 15x12x12x10\n",
        "            cur_state = current_state_list[i]   # 12x12x6\n",
        "\n",
        "            cur_state_expanded = np.zeros(shape=(self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH))\n",
        "            cur_state_expanded[..., 0:6] = cur_state # 12x12x6 -> 12x12x10\n",
        "\n",
        "            Ntraj = cur_traj.shape[0]\n",
        "            concat_shape = (Ntraj + 1, self.MAZE_WIDTH, self.MAZE_HEIGHT, self.MAZE_DEPTH)\n",
        "            concatenated_data = np.zeros(shape=concat_shape)\n",
        "            concatenated_data[0:Ntraj] = cur_traj\n",
        "            concatenated_data[Ntraj] = cur_state_expanded\n",
        "\n",
        "            united_data.append(concatenated_data)\n",
        "\n",
        "        single_game[\"ToM\"][\"united_input\"] = united_data\n",
        "\n",
        "        return single_game\n",
        "\n",
        "\n",
        "    def validate_data(self, DictData):\n",
        "\n",
        "        \"\"\"\n",
        "        Data = {\"train_traj\":train_traj,\n",
        "                \"test_traj\":test_traj,\n",
        "                \"valid_traj\":valid_traj,\n",
        "                \"train_current\":train_current,\n",
        "                \"test_current\":train_current,\n",
        "                \"valid_current\":valid_current,\n",
        "                \"train_goal\": train_goal,\n",
        "                \"test_goal\": test_goal,\n",
        "                \"valid_goal\": valid_goal,\n",
        "                \"train_act\": train_act,\n",
        "                \"test_act\": test_act,\n",
        "                \"valid_act\": valid_act}\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"----\")\n",
        "        print(\"Data validation... \")\n",
        "\n",
        "        for key, value in DictData.items():\n",
        "\n",
        "            if key[-len(\"traj\"):] == \"traj\":\n",
        "                if key == \"train_traj\":\n",
        "                    self.trajectory_validation(value)\n",
        "            elif key[-len(\"current\"):] == \"current\":\n",
        "                self.current_validation(value)\n",
        "            elif key[-len(\"goal\"):] == \"goal\":\n",
        "                self.goal_validation(value)\n",
        "            elif key[-len(\"act\"):] == \"act\":\n",
        "                self.act_validation(value)\n",
        "            else:\n",
        "                raise ValueError(\"Wrong key inside Data dictionary!\")\n",
        "\n",
        "        print(\"----\")\n",
        "\n",
        "    def one_trajectory_validation(self, traj):\n",
        "\n",
        "        for i in range(len(traj)):\n",
        "            # Take i-th frame of trajectory\n",
        "            frame_1 = traj[i]\n",
        "\n",
        "            walls = frame_1[..., 0]\n",
        "            player = frame_1[..., 1]\n",
        "            goal1 = frame_1[..., 2]\n",
        "            goal2 = frame_1[..., 3]\n",
        "            goal3 = frame_1[..., 4]\n",
        "            goal4 = frame_1[..., 5]\n",
        "            act1 = frame_1[..., 6]\n",
        "            act2 = frame_1[..., 7]\n",
        "            act3 = frame_1[..., 8]\n",
        "            act4 = frame_1[..., 9]\n",
        "\n",
        "            to_draw = {\n",
        "                \"walls\": walls,\n",
        "                \"player\": player,\n",
        "                \"walls2\": walls,\n",
        "                \"player2\": player,\n",
        "                \"goal1\": goal1,\n",
        "                \"goal2\": goal2,\n",
        "                \"goal3\": goal3,\n",
        "                \"goal4\": goal4,\n",
        "                \"act1(UP)\": act1,\n",
        "                \"act2(RIGHT)\": act2,\n",
        "                \"act3(DOWN)\": act3,\n",
        "                \"act4(LEFT)\": act4\n",
        "            }\n",
        "\n",
        "            ROW = 3\n",
        "            COL = 4\n",
        "            fig, axs = plt.subplots(ROW, COL, figsize=(7, 6))\n",
        "            row = 0\n",
        "            col = 0\n",
        "            for key, value in to_draw.items():\n",
        "                axs[row, col].imshow(value)\n",
        "                axs[row, col].set_title(key + \"::\" + str(i))\n",
        "                axs[row, col].axis(\"off\")\n",
        "                col = col + 1\n",
        "                if col == COL:\n",
        "                    col = 0\n",
        "                    row = row + 1\n",
        "            plt.show()\n",
        "\n",
        "    def trajectory_validation(self, traj):\n",
        "        print(\"Trajectory validation... \")\n",
        "\n",
        "        for index, tau in enumerate(traj):\n",
        "            if index == 0:\n",
        "\n",
        "                for i in range(tau.shape[0]):\n",
        "\n",
        "                    # Take i-th frame of trajectory\n",
        "                    frame_1 = tau[i]\n",
        "\n",
        "                    walls = frame_1[..., 0]\n",
        "                    player = frame_1[..., 1]\n",
        "                    goal1 = frame_1[..., 2]\n",
        "                    goal2 = frame_1[..., 3]\n",
        "                    goal3 = frame_1[..., 4]\n",
        "                    goal4 = frame_1[..., 5]\n",
        "                    act1 = frame_1[..., 6]\n",
        "                    act2 = frame_1[..., 7]\n",
        "                    act3 = frame_1[..., 8]\n",
        "                    act4 = frame_1[..., 9]\n",
        "\n",
        "                    fig, ax = plt.subplot_mosaic([\n",
        "                        [\"walls\",  \"player\"],\n",
        "                        [\"goal 1\", \"goal 2\"],\n",
        "                        [\"goal 3\", \"goal 4\"],\n",
        "                        [\"act 1\",  \"act 2\"],\n",
        "                        [\"act 3\", \"act 4\"]\n",
        "                    ], constrained_layout=True)\n",
        "\n",
        "                    # Draw walls\n",
        "                    ax[\"walls\"].set_title(\"Walls-\" + str(i))\n",
        "                    ax[\"walls\"].imshow(walls)\n",
        "\n",
        "                    # Draw Player\n",
        "                    ax[\"player\"].set_title(\"Player-\" + str(i))\n",
        "                    ax[\"player\"].imshow(player)\n",
        "\n",
        "                    # Draw Goal 1\n",
        "                    ax[\"goal 1\"].set_title(\"Goal 1-\" + str(i))\n",
        "                    ax[\"goal 1\"].imshow(goal1)\n",
        "\n",
        "                    # Draw Goal 2\n",
        "                    ax[\"goal 2\"].set_title(\"Goal 2-\" + str(i))\n",
        "                    ax[\"goal 2\"].imshow(goal2)\n",
        "\n",
        "                    # Draw Goal 3\n",
        "                    ax[\"goal 3\"].set_title(\"Goal 3-\" + str(i))\n",
        "                    ax[\"goal 3\"].imshow(goal3)\n",
        "\n",
        "                    # Draw Goal 4\n",
        "                    ax[\"goal 4\"].set_title(\"Goal 4-\" + str(i))\n",
        "                    ax[\"goal 4\"].imshow(goal4)\n",
        "\n",
        "                    # Draw Action 1\n",
        "                    ax[\"act 1\"].set_title(\"Action 1-\" + str(i))\n",
        "                    ax[\"act 1\"].imshow(act1)\n",
        "\n",
        "                    # Draw Action 2\n",
        "                    ax[\"act 2\"].set_title(\"Action 2-\" + str(i))\n",
        "                    ax[\"act 2\"].imshow(act2)\n",
        "\n",
        "                    # Draw Action 3\n",
        "                    ax[\"act 3\"].set_title(\"Action 3-\" + str(i))\n",
        "                    ax[\"act 3\"].imshow(act3)\n",
        "\n",
        "                    # Draw Action 4\n",
        "                    ax[\"act 4\"].set_title(\"Action 4-\" + str(i))\n",
        "                    ax[\"act 4\"].imshow(act4)\n",
        "\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def current_validation(self, cur):\n",
        "        print(\"Current state validation... \")\n",
        "\n",
        "        to_draw = {\n",
        "            \"walls\": cur[..., 0],\n",
        "            \"player\": cur[..., 1],\n",
        "            \"walls2\": cur[..., 0],\n",
        "            \"player2\": cur[..., 1],\n",
        "            \"goal1\": cur[..., 2],\n",
        "            \"goal2\": cur[..., 3],\n",
        "            \"goal3\": cur[..., 4],\n",
        "            \"goal4\": cur[..., 5],\n",
        "        }\n",
        "\n",
        "        ROW = 2\n",
        "        COL = 4\n",
        "        fig, axs = plt.subplots(ROW, COL, figsize=(7, 6))\n",
        "        row = 0\n",
        "        col = 0\n",
        "        for key, value in to_draw.items():\n",
        "            axs[row, col].imshow(value)\n",
        "            axs[row, col].set_title(key + \":: Current State\")\n",
        "            axs[row, col].axis(\"off\")\n",
        "            col = col + 1\n",
        "            if col == COL:\n",
        "                col = 0\n",
        "                row = row + 1\n",
        "        plt.show()\n",
        "\n",
        "    def goal_validation(self, traj):\n",
        "        print(\"Goal validation... \")\n",
        "\n",
        "    def act_validation(self, traj):\n",
        "        print(\"Action validation... \")"
      ],
      "metadata": {
        "id": "0iThOe6qAAcj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "7CLswwYrAGRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main functions"
      ],
      "metadata": {
        "id": "LeQYwrVLAMwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "class Model(mp.ModelParameter):\n",
        "\n",
        "The class for training the ToMNET model.\n",
        "\n",
        "Note:\n",
        "  Inherit mp.ModelParameter to share model constants.\n",
        "\n",
        "@author: Chuang, Yun-Shiuan; Edwinn\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# CONSTANTS and Parameters\n",
        "# --------------------------------------------------------\n",
        "N_ECHAR = 8\n",
        "N_RESBLOCKS = 64\n",
        "LEARNING_RATE = 0.0001 / 5\n",
        "BATCH_SIZE = 32\n",
        "ROW = 12\n",
        "COL = 12\n",
        "DEPTH = 10\n",
        "MAX_TRAJ = 15\n",
        "EPOCHS = 15 # 150 (no need to have more than 150)\n",
        "\n",
        "LOAD_PERCENTAGE = 0.001 # 0.1% = 5 games. 0.02% = 1 game\n",
        "\n",
        "MODEL_PATH = os.path.join('drive', 'MyDrive', 'Dissertation', 'Games', 'Model')\n",
        "TESTING_GAME_PATH = os.path.join('drive', 'MyDrive', 'Dissertation', 'Games', 'Overfit')\n",
        "TRAINING_GAMES_PATH = os.path.join('drive', 'MyDrive', 'Dissertation', 'Games', 'Experiment 2')\n",
        "\n",
        "def dict_to_tensors(Dict):\n",
        "\n",
        "    def make_y_outputs(folded_list):\n",
        "        list_of_arrays = folded_list\n",
        "        indices = list(np.concatenate([list_of_arrays], axis=0))\n",
        "        indices = [x - 1 for x in indices]  # 1-4 --> 0-3\n",
        "        depth = 4\n",
        "        return tf.one_hot(indices, depth)\n",
        "\n",
        "    X_Train = tf.convert_to_tensor(Dict[\"train_input\"])\n",
        "    X_Test = tf.convert_to_tensor(Dict[\"test_input\"])\n",
        "    X_Valid = tf.convert_to_tensor(Dict[\"valid_input\"])\n",
        "\n",
        "    Y_goal_Train = make_y_outputs(Dict[\"train_goal\"])\n",
        "    Y_goal_Test = make_y_outputs(Dict[\"test_goal\"])\n",
        "    Y_goal_Valid = make_y_outputs(Dict[\"valid_goal\"])\n",
        "\n",
        "    Y_act_Train = make_y_outputs(Dict[\"train_act\"])\n",
        "    Y_act_Test = make_y_outputs(Dict[\"test_act\"])\n",
        "    Y_act_Valid = make_y_outputs(Dict[\"valid_act\"])\n",
        "\n",
        "    return X_Train, X_Test, X_Valid, \\\n",
        "           Y_goal_Train, Y_goal_Test, Y_goal_Valid, \\\n",
        "           Y_act_Train, Y_act_Test, Y_act_Valid,\n",
        "\n",
        "def save_game_to_draw(full_trajectory, predicted_actions):\n",
        "    print(\"Puk-puk\")\n",
        "\n",
        "def load_training_games(directory, load_percentage=0.2):\n",
        "    # --------------------------------------------------------\n",
        "    # 1. Load Data\n",
        "    # --------------------------------------------------------\n",
        "    data_handler = DataHandler(ts=MAX_TRAJ,\n",
        "                              w=ROW,\n",
        "                              h=COL,\n",
        "                              d=DEPTH)\n",
        "    #\n",
        "    # all_games = {\n",
        "    #     \"traj_history\": traj_history,\n",
        "    #     \"traj_history_zp\": traj_history_zp                    # Trajectory with Zero Padding\n",
        "    #     \"current_state_history\": current_state_history,\n",
        "    #     \"actions_history\": actions_history\n",
        "    # }\n",
        "    all_games = data_handler.load_all_games_v2(directory=directory, use_percentage=load_percentage)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 2. Pre-process data - Zero Padding\n",
        "    # --------------------------------------------------------\n",
        "    data_processor = DataProcessor(ts=MAX_TRAJ,\n",
        "                                    w=ROW,\n",
        "                                    h=COL,\n",
        "                                    d=DEPTH)\n",
        "\n",
        "    all_games = data_processor.zero_padding_v2(max_elements=MAX_TRAJ,\n",
        "                                                all_games=all_games)\n",
        "\n",
        "    # Make Tensors from List\n",
        "    indices = all_games[\"actions_history\"] # In Experiment 2 actions already saved like 0-3. For Experiment 1 use -> -1 ##########- 1  # 1-4 --> 0-3\n",
        "    depth = 4\n",
        "    X_train_traj = tf.convert_to_tensor(all_games[\"traj_history_zp\"], dtype=tf.float32)\n",
        "    X_train_current = tf.convert_to_tensor(all_games[\"current_state_history\"], dtype=tf.float32)\n",
        "    Y_act_Train = tf.one_hot(indices, depth)\n",
        "\n",
        "    # return X_Train, Y_act_Train\n",
        "    return X_train_traj, X_train_current, Y_act_Train\n",
        "\n",
        "def load_one_game(directory):\n",
        "    # --------------------------------------------------------\n",
        "    # 1. Load Data\n",
        "    # --------------------------------------------------------\n",
        "    data_handler = DataHandler(ts=MAX_TRAJ,\n",
        "                                          w=ROW,\n",
        "                                          h=COL,\n",
        "                                          d=DEPTH)\n",
        "    #\n",
        "    # single_game = {\n",
        "    #     \"traj\": traj,  # Original trajectory\n",
        "    #     \"act\": act,\n",
        "    #     \"goal\": goal,\n",
        "    #     \"ToM\":\n",
        "    #         {\n",
        "    #             \"traj_history\": traj_history,\n",
        "    #             \"traj_history_zp\": traj_history_zp # Trajectory with Zero Padding\n",
        "    #             \"current_state_history\": current_state_history,\n",
        "    #             \"actions_history\": actions_history\n",
        "    #         }\n",
        "    # }\n",
        "    single_game = data_handler.load_one_game(directory=directory)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 2. Pre-process data - Zero Padding\n",
        "    # --------------------------------------------------------\n",
        "    data_processor = DataProcessor(ts=MAX_TRAJ,\n",
        "                                    w=ROW,\n",
        "                                    h=COL,\n",
        "                                    d=DEPTH)\n",
        "\n",
        "    # data_processor.validate_data(Data)\n",
        "\n",
        "    # single_game = data_processor.unite_single_traj_current(single_game)\n",
        "\n",
        "    single_game = data_processor.zero_pad_single_game(max_elements=MAX_TRAJ,\n",
        "                                                      single_game=single_game)\n",
        "\n",
        "    # Make Tensors from List\n",
        "    indices = [x - 1 for x in single_game[\"ToM\"][\"actions_history\"]]  # 1-4 --> 0-3\n",
        "    depth = 4\n",
        "    X_train_traj = tf.convert_to_tensor(single_game[\"ToM\"][\"traj_history_zp\"], dtype=tf.float32)\n",
        "    X_train_current = tf.convert_to_tensor(single_game[\"ToM\"][\"current_state_history\"], dtype=tf.float32)\n",
        "    Y_act_Train = tf.one_hot(indices, depth)\n",
        "\n",
        "    # return X_Train, Y_act_Train\n",
        "    return X_train_traj, X_train_current, Y_act_Train\n",
        "\n",
        "def train_model(X_train_traj, X_train_current, Y_act_Train):\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 3. Create and set the model\n",
        "    # --------------------------------------------------------\n",
        "    print(\"----\")\n",
        "    print(\"Create a model\")\n",
        "\n",
        "    t = ToMnet(ts=MAX_TRAJ,\n",
        "                      w=ROW,\n",
        "                      h=COL,\n",
        "                      d=DEPTH,\n",
        "                      Ne_char=N_ECHAR,\n",
        "                      N_res_blocks=N_RESBLOCKS,\n",
        "                      filters=64)\n",
        "    t.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(LEARNING_RATE, clipnorm=1.0),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    t.fit(x=[X_train_traj, X_train_current], y=Y_act_Train,\n",
        "          epochs=1, batch_size=BATCH_SIZE, verbose=2)\n",
        "\n",
        "    t.summary()\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 4. Train the model\n",
        "    # --------------------------------------------------------\n",
        "    print(\"X_traj shape: \", X_train_traj.shape)\n",
        "    print(\"Train a Model\")\n",
        "    history = t.fit(x=[X_train_traj, X_train_current], y=Y_act_Train,\n",
        "                    epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
        "    plot_history(history)\n",
        "    save_history(history)\n",
        "\n",
        "    t.save(MODEL_PATH)\n",
        "\n",
        "def plot_history(history):\n",
        "\n",
        "    plt.plot(\n",
        "        np.arange(1, EPOCHS + 1),\n",
        "        history.history['loss'],\n",
        "        label='Loss', lw=3\n",
        "    )\n",
        "    plt.plot(\n",
        "        np.arange(1, EPOCHS + 1),\n",
        "        history.history['accuracy'],\n",
        "        label='Accuracy', lw=3\n",
        "    )\n",
        "\n",
        "    plt.title('Evaluation metrics', size=20)\n",
        "    plt.xlabel('Epoch', size=14)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(\n",
        "        np.arange(1, EPOCHS + 1),\n",
        "        history.history['loss'],\n",
        "        label='Loss', lw=3\n",
        "    )\n",
        "    plt.plot(\n",
        "        np.arange(1, EPOCHS + 1),\n",
        "        history.history['accuracy'],\n",
        "        label='Accuracy', lw=3\n",
        "    )\n",
        "\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title('Evaluation metrics', size=20)\n",
        "    plt.xlabel('Epoch', size=14)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def save_history(history, name=\"TrainHistory.csv\"):\n",
        "    TrainHistory = pd.DataFrame()\n",
        "    TrainHistory = TrainHistory.append(pd.DataFrame({\n",
        "        \"loss\": history.history['loss'],\n",
        "        \"accuracy\": history.history['accuracy']\n",
        "    }))\n",
        "    TrainHistory.to_csv(name)\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    custom_layers = {\n",
        "        \"CustomCnn\": CustomCnn,\n",
        "        \"ResBlock\": ResBlock,\n",
        "        \"CustomLSTM\": CustomLSTM,\n",
        "    }\n",
        "    return tf.keras.models.load_model(MODEL_PATH, custom_objects=custom_layers)\n",
        "\n",
        "# I am only sending a trajectory here\n",
        "# This trajectory will be divided to Traj and Current state\n",
        "# And therefore coordinates will be calculated\n",
        "def predict_game(model, input_data, predict_steps=5):\n",
        "\n",
        "    # Trajectory Depth saved as - (np_obstacles, np_agent, np_targets, np_actions)\n",
        "    # Traj shape = BS x TS x W x H x D. 1x5-15x12x12x10\n",
        "\n",
        "    # Check for Batch_Size dim\n",
        "    if input_data.shape[0] != 1:\n",
        "        input_data = tf.expand_dims(input_data, axis=0)\n",
        "\n",
        "    path_to_save = \"../Results/Predictions/Prediction 1/\"\n",
        "    width = input_data.shape[2]\n",
        "    height = input_data.shape[3]\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 1. Build Initial Map (simple map) for rendering\n",
        "    # --------------------------------------------------------\n",
        "    simple_map = np.zeros((12, 12), dtype=np.int16)  # 0-path, 1-wall, 2/5-goals, 10-player\n",
        "\n",
        "    # Put walls\n",
        "    walls_layer = input_data[0, 0, ..., 0]\n",
        "    for row in range(width):\n",
        "        for col in range(height):\n",
        "            if walls_layer[row, col] == 1:\n",
        "                simple_map[row, col] = 1\n",
        "\n",
        "    # Put player\n",
        "    player_layer = input_data[0, 0, ..., 1]\n",
        "    for row in range(width):\n",
        "        for col in range(height):\n",
        "            if player_layer[row, col] == 1:\n",
        "                simple_map[row, col] = 10\n",
        "\n",
        "    # Put goals\n",
        "    goal_layer = input_data[0, 0, ..., 2:6]\n",
        "    assert goal_layer.shape[-1] == 4            # Check that there are 4 layers for 4 goals\n",
        "    for row in range(width):\n",
        "        for col in range(height):\n",
        "            if goal_layer[row, col, 0] == 1:\n",
        "                simple_map[row, col] = 2\n",
        "            elif goal_layer[row, col, 1] == 1:\n",
        "                simple_map[row, col] = 3\n",
        "            elif goal_layer[row, col, 2] == 1:\n",
        "                simple_map[row, col] = 4\n",
        "            elif goal_layer[row, col, 3] == 1:\n",
        "                simple_map[row, col] = 5\n",
        "    map_df = pd.DataFrame(simple_map)\n",
        "    map_df.to_csv(path_to_save + str(\"simple_map.csv\"))\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 2. Save Full Trajectory\n",
        "    # --------------------------------------------------------\n",
        "    full_traj_actions = []\n",
        "\n",
        "    # Create list of actions saved in trajectory\n",
        "    TS = input_data.shape[1] - 1   # Trajectory Size. The last frame is current state, no actions are shown there\n",
        "    for i in range(TS):\n",
        "        all_action_layers = np.array(input_data[0, i, ..., 6:10])\n",
        "\n",
        "        # Find which action was performed\n",
        "        bool_val = False\n",
        "        for action_number in range(4):\n",
        "            action_layer = np.array(all_action_layers[..., action_number], dtype=np.int8)\n",
        "            max_val = action_layer.max()\n",
        "            bool_val = 1 in action_layer    # np.where(n_array == 1) # Should also work\n",
        "            bool_val = np.any(bool_val)\n",
        "            if bool_val:\n",
        "                full_traj_actions.append(action_number)\n",
        "                break\n",
        "\n",
        "        # If no actions were found in a frame - it is a Zero_padding. Finish here\n",
        "        if not bool_val:\n",
        "            TS = i\n",
        "            break\n",
        "\n",
        "    print(\"Full trajectory in actions: \", full_traj_actions)\n",
        "\n",
        "    # Find initial position\n",
        "    initial_coordinates = list(np.where(player_layer == 1))\n",
        "\n",
        "    # Create coordinate sequence\n",
        "    full_traj_coordinates = [initial_coordinates]\n",
        "    for i in range(TS):\n",
        "        coordinates = full_traj_coordinates[-1].copy()\n",
        "        applied_action = full_traj_actions[i]\n",
        "\n",
        "        dr = 0\n",
        "        dc = 0\n",
        "        if applied_action == 0:\n",
        "            dr = -1\n",
        "            dc = 0\n",
        "        elif applied_action == 1:\n",
        "            dr = 0\n",
        "            dc = 1\n",
        "        elif applied_action == 2:\n",
        "            dr = 1\n",
        "            dc = 0\n",
        "        elif applied_action == 3:\n",
        "            dr = 0\n",
        "            dc = -1\n",
        "\n",
        "        coordinates[0] = coordinates[0] + dr\n",
        "        coordinates[1] = coordinates[1] + dc\n",
        "\n",
        "        full_traj_coordinates.append(coordinates)\n",
        "\n",
        "    print(\"Full trajectory in coordinates: \", full_traj_coordinates)\n",
        "    full_traj_coordinates_df = pd.DataFrame(full_traj_coordinates)\n",
        "    full_traj_coordinates_df.to_csv(path_to_save + str(\"full_traj.csv\"))\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 3. Save Initial Trajectory\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    Nfull = len(full_traj_coordinates)\n",
        "    if Nfull - predict_steps < 5:\n",
        "        raise ValueError(\"The game is too short! It has only \" + str(Nfull) + \" moves, while you ask to predict\"\n",
        "                         + str(predict_steps) +\n",
        "                         \" actions. Give at least a game with trajectory length bigger than predicted actions by 5.\")\n",
        "    initial_traj_coordinates = full_traj_coordinates[0:-predict_steps]\n",
        "    initial_traj_coordinates_df = pd.DataFrame(initial_traj_coordinates)\n",
        "    initial_traj_coordinates_df.to_csv(path_to_save + str(\"init_traj.csv\"))\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 4. Save Predicted Trajectory\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    # Remove Zero-Padding from Trajectory\n",
        "    # ...\n",
        "    # Currently it doesn't have zero-padding\n",
        "\n",
        "    # Traj -> Traj_zero_pad + current state\n",
        "    input_traj = tf.cast(input_data[0, 0:-predict_steps, ...], tf.float32)\n",
        "    input_current = tf.cast(input_data[0, -predict_steps, ..., 0:6], tf.float32)\n",
        "    NeededZeros = MAX_TRAJ - input_traj.shape[0] # Add Zeros up to MAX_TRAJ\n",
        "    if NeededZeros > 0:\n",
        "        zero_pad_shape = (NeededZeros, ROW, COL, DEPTH)\n",
        "        zero_pad = tf.zeros(shape=zero_pad_shape, dtype=tf.float32)\n",
        "        input_traj = tf.concat(values=[input_traj, zero_pad], axis=0)\n",
        "    input_traj = tf.expand_dims(input_traj, axis=0)\n",
        "    input_current = tf.expand_dims(input_current, axis=0)\n",
        "\n",
        "    input_traj    = tf.cast(input_traj, tf.float32)\n",
        "    input_current = tf.cast(input_current, tf.float32)\n",
        "\n",
        "    # Get initial coordinates\n",
        "    np_input_current = input_current.numpy()\n",
        "    np_player_postition = np_input_current[0, ..., 1]\n",
        "    position = list(np.where(np_player_postition == np_player_postition.max()))\n",
        "\n",
        "    # Make action predictions\n",
        "    predicted_actions = []\n",
        "    current_player_coordinates = initial_traj_coordinates[-1].copy()\n",
        "    coordinates = [position] # [current_player_coordinates]\n",
        "    for i in range(predict_steps):\n",
        "        # Get predicted action\n",
        "        predict_distribution = model.predict([input_traj, input_current])\n",
        "        predicted_action = np.where(predict_distribution == predict_distribution.max())[1][0]   # Output: 0 - 3\n",
        "        predicted_actions.append(predicted_action)\n",
        "        print(\"Predicted Action: \", predicted_action)\n",
        "\n",
        "        np_input_traj = input_traj.numpy()\n",
        "        np_input_current = input_current.numpy()\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        # 4.1 Update layers\n",
        "        # --------------------------------------------------------\n",
        "\n",
        "        # Update players coordinates\n",
        "        old_player_position = coordinates[-1].copy()    # current_player_coordinates.copy()\n",
        "        player_position = old_player_position.copy()\n",
        "        if predicted_action == 0:    player_position[0] = player_position[0] - 1\n",
        "        elif predicted_action == 1:  player_position[1] = player_position[1] + 1\n",
        "        elif predicted_action == 2:  player_position[0] = player_position[0] + 1\n",
        "        elif predicted_action == 3:  player_position[1] = player_position[1] - 1\n",
        "\n",
        "        # Check for safety (map boundaries)\n",
        "        if player_position[0] > ROW-1: player_position[0] = ROW-1\n",
        "        if player_position[0] < 0: player_position[0] = 0\n",
        "        if player_position[1] > COL-1: player_position[1] = COL-1\n",
        "        if player_position[1] < 0: player_position[1] = 0\n",
        "\n",
        "        current_player_coordinates = player_position.copy()\n",
        "        coordinates.append(current_player_coordinates)\n",
        "\n",
        "        new_player_map = np.zeros(shape=(ROW, COL, 1))\n",
        "        new_player_map[player_position[0], player_position[1], 0] = 1\n",
        "        # new_player_map = tf.convert_to_tensor(new_player_map, dtype=tf.float32)\n",
        "\n",
        "        # Update action layers (ACTION IS ASSIGNED TO CURRENT FRAME, NOT NEW FRAME!!! So it takes old player's position)\n",
        "        # The old position is saved in Current State\n",
        "        action_map = np.zeros(shape=(ROW, COL, 4))\n",
        "        action_map[old_player_position[0], old_player_position[1], predicted_action] = 1\n",
        "        # old_action_map = tf.convert_to_tensor(old_action_map, dtype=tf.float32)\n",
        "\n",
        "        # Get wall layer\n",
        "        wall_map = input_traj[0, 0, ..., 0]\n",
        "        wall_map = tf.expand_dims(wall_map, axis=-1)\n",
        "        np_wall_map = wall_map.numpy()\n",
        "\n",
        "        # Get goals map\n",
        "        goal_map = input_traj[0, 0, ..., 2:6]\n",
        "        np_goal_map = goal_map.numpy()\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        # 4.2 Update Trajectory\n",
        "        # --------------------------------------------------------\n",
        "        # Indexes:  traj1, traj2, traj3, traj4, traj5, zp,    zp, zp, zp, zp\n",
        "        # Become:   traj1, traj2, traj3, traj4, traj5, traj6, zp, zp, zp, zp\n",
        "        # Initial number of zp = predict_steps\n",
        "        # It decreases with increasing of i\n",
        "        # Therefore I must update input_traj[0, -predict_step+i, ...]\n",
        "\n",
        "        np_current_state = np.copy(input_current.numpy())\n",
        "        np_current_state = np_current_state[0, ...]   # Here I have walls, player and goals for a trajectory frame\n",
        "\n",
        "        upd_ind = -predict_steps+i\n",
        "        np_input_traj = input_traj.numpy()\n",
        "        np_input_traj[0, upd_ind, ...] = np.concatenate((np_current_state, action_map), axis=-1)\n",
        "        input_traj = tf.convert_to_tensor(np_input_traj, dtype=tf.float32)\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        # 4.3 Update Current State\n",
        "        # --------------------------------------------------------\n",
        "\n",
        "        new_current_state = np.concatenate((np_wall_map, new_player_map, np_goal_map), axis=-1)\n",
        "        new_current_state = tf.convert_to_tensor(new_current_state, dtype=tf.float32)\n",
        "        new_current_state = tf.expand_dims(new_current_state, axis=0)\n",
        "\n",
        "        input_current = new_current_state\n",
        "\n",
        "    coordinates_df = pd.DataFrame(coordinates)\n",
        "    coordinates_df.to_csv(path_to_save + str(\"predicted_traj.csv\"))\n",
        "\n",
        "    return predicted_actions"
      ],
      "metadata": {
        "id": "sEKTNNVAAROQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional function for Google Colab"
      ],
      "metadata": {
        "id": "m6TxM2X1AqOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UvPbxelAt6Q",
        "outputId": "c2fa15c9-2890-4e01-c372-c9d540689fbc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfoWVb00BAQU",
        "outputId": "0e415dd4-f8c3-4a81-cc10-6f0050fe982b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Script"
      ],
      "metadata": {
        "id": "SuXotF4AATws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # To fix ERROR\n",
        "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "\n",
        "    ### Load data\n",
        "    X_train_traj, X_train_current, Y_act_Train = load_training_games(directory=TRAINING_GAMES_PATH,\n",
        "                                                                     load_percentage=LOAD_PERCENTAGE)\n",
        "\n",
        "    print(Y_act_Train)\n",
        "    ### Train the model\n",
        "    train_model(X_train_traj, X_train_current, Y_act_Train)\n",
        "\n",
        "    ### Use trained model\n",
        "    # model =  load_model()\n",
        "    #\n",
        "    # ### Keep training the model\n",
        "    # # history = model.fit(x=X_Train, y=Y_act_Train,\n",
        "    # #                 epochs=EPOCHS, batch_size=1, verbose=2)\n",
        "    # # plot_history(history)\n",
        "    #\n",
        "    # ### Test it on one prediction\n",
        "    # X_train_traj, X_train_current, Y_act_Train = load_one_game(directory=TESTING_GAME_PATH) # To test I load one single game\n",
        "    #\n",
        "    # # Pick the longest trajectory, which has 14 moves and 15tf frame is current state\n",
        "    # input_data_traj = X_train_traj[-6, ...]\n",
        "    # input_data_current = X_train_current[-6, ...]\n",
        "    # input_data_traj = tf.expand_dims(input_data_traj, axis=0)  # Add axis for \"batch_size\"\n",
        "    # input_data_current = tf.expand_dims(input_data_current, axis=0)  # Add axis for \"batch_size\"\n",
        "    # actual_action = Y_act_Train[-6]\n",
        "    # yhat = model.predict([input_data_traj, input_data_current])\n",
        "    #\n",
        "    # print(\"Testing prediction:\")\n",
        "    # print(\"Actual action: \", actual_action)\n",
        "    # print(\"Predicted action: \", yhat)\n",
        "    #\n",
        "    # ### Predict trajectory\n",
        "    # print(\"Predict Trajectory:\")\n",
        "    # predict_game(model, input_data_traj, predict_steps=5)\n",
        "\n",
        "    print(\"------------------------------------\")\n",
        "    print(\"Congratultions! You have reached the end of the script.\")\n",
        "    print(\"------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "5x2QagaJATa9",
        "outputId": "a4d678fc-68c5-4529-bfc3-7688c3629152"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0be26fcac78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m### Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     X_train_traj, X_train_current, Y_act_Train = load_training_games(directory=TRAINING_GAMES_PATH,\n\u001b[0;32m----> 8\u001b[0;31m                                                                      load_percentage=LOAD_PERCENTAGE)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_act_Train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-764ce04da639>\u001b[0m in \u001b[0;36mload_training_games\u001b[0;34m(directory, load_percentage)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m#     \"actions_history\": actions_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mall_games\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_all_games_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_percentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_percentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# --------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f4a9967f6482>\u001b[0m in \u001b[0;36mload_all_games_v2\u001b[0;34m(self, directory, use_percentage)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Get names of games\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".*.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/Dissertation/Games/Experiment 2'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "uIkTFi5nKVIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}